{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getenv('PYTHONPATH')) \n",
    "import pandas as pd\n",
    "import os\n",
    "import tqdm as tqdm\n",
    "import glob as glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = os.getenv(\"DATASETS_ROOT\", \"/default/path/to/datasets\") #use default if DATASETS_ROOT env variable is not set.\n",
    "print(f\"dataset_root: {dataset_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 01: load each dataset's stiminfo file\n",
    "nsd_stiminfo = pd.read_table(os.path.join(dataset_root, \"MOSAIC\", \"stimuli\", \"datasets_stiminfo\", \"nsd_stiminfo.tsv\"))\n",
    "bmd_stiminfo = pd.read_table(os.path.join(dataset_root, \"MOSAIC\", \"stimuli\", \"datasets_stiminfo\", \"bmd_stiminfo.tsv\"))\n",
    "b5000_stiminfo = pd.read_table(os.path.join(dataset_root, \"MOSAIC\", \"stimuli\", \"datasets_stiminfo\", \"b5000_stiminfo.tsv\"))\n",
    "things_stiminfo = pd.read_table(os.path.join(dataset_root, \"MOSAIC\", \"stimuli\", \"datasets_stiminfo\", \"things_stiminfo.tsv\"))\n",
    "god_stiminfo = pd.read_table(os.path.join(dataset_root, \"MOSAIC\", \"stimuli\", \"datasets_stiminfo\", \"god_stiminfo.tsv\"))\n",
    "deeprecon_stiminfo = pd.read_table(os.path.join(dataset_root, \"MOSAIC\", \"stimuli\", \"datasets_stiminfo\", \"deeprecon_stiminfo.tsv\"))\n",
    "had_stiminfo = pd.read_table(os.path.join(dataset_root, \"MOSAIC\", \"stimuli\", \"datasets_stiminfo\", \"had_stiminfo.tsv\"))\n",
    "nod_stiminfo = pd.read_table(os.path.join(dataset_root, \"MOSAIC\", \"stimuli\", \"datasets_stiminfo\", \"nod_stiminfo.tsv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'NSD': nsd_stiminfo,\n",
    "    'BMD':bmd_stiminfo,\n",
    "    'BOLD5000':b5000_stiminfo,\n",
    "    'THINGS':things_stiminfo,\n",
    "    'GOD':god_stiminfo,\n",
    "    'deeprecon':deeprecon_stiminfo,\n",
    "    'HAD':had_stiminfo,\n",
    "    'NOD':nod_stiminfo,\n",
    "}\n",
    "#base dataset is NSD. arbitrary choice, doesnt matter\n",
    "base_dataset = 'NSD'\n",
    "merged_df = datasets['NSD'].rename(columns={col: f\"{col}_{base_dataset}\" for col in datasets[base_dataset].columns if col != 'filename'})\n",
    "\n",
    "for dataset_name, df in datasets.items():\n",
    "    if dataset_name == 'NSD':\n",
    "        continue  #skip base dataset\n",
    "\n",
    "    #rename columns to make it dataset specific\n",
    "    df = datasets[dataset_name].rename(columns={col: f\"{col}_{dataset_name}\" for col in datasets[dataset_name].columns if col != 'filename'})\n",
    "    merged_df = pd.merge(merged_df, df, on='filename', how='outer')  # use 'outer' to keep all filenames\n",
    "\n",
    "pattern = r\"sub-.*_reps.*\"\n",
    "repetition_columns = [col for col in merged_df.columns if re.search(pattern, col)]\n",
    "\n",
    "merged_df[repetition_columns] = merged_df[repetition_columns].fillna(0) #fill nans as 0\n",
    "merged_df[repetition_columns] = merged_df[repetition_columns].astype(int) #change float values to int for the subject repetition columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge columns that should be the same\n",
    "source = [] #get the source of each filename and ensure no conflicts\n",
    "for idx, filename in enumerate(merged_df['filename']):\n",
    "    possible_sources = set()\n",
    "    for dataset_name in datasets.keys(): #loop over the datasets\n",
    "        if f'source' in datasets[dataset_name].columns: #if the column exists for this dataset\n",
    "            possible_source = merged_df.loc[idx, f'source_{dataset_name}'] #get whatever this dataset says is the filenames source\n",
    "            if pd.notna(possible_source): #if the source is not nan\n",
    "                if possible_source not in possible_sources: #and if the possible source has not been added already\n",
    "                    possible_sources.add(possible_source)\n",
    "    if len(possible_sources) == 1:\n",
    "        source.append(list(possible_sources)[0])\n",
    "    elif len(possible_sources) > 1:\n",
    "        raise ValueError(f\"This filename {idx} {filename} has multiple possible sources: {possible_sources}. It should have just one.\")\n",
    "    else:\n",
    "        source.append(None)\n",
    "merged_df['source'] = source\n",
    "#drop the dataset-specific source columns\n",
    "for dataset_name in datasets.keys(): #loop over the datasets\n",
    "    if f'source' in datasets[dataset_name].columns: \n",
    "        merged_df = merged_df.drop(f'source_{dataset_name}', axis=1)\n",
    "print(merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = {} #get the source of each filename and ensure no conflicts\n",
    "for idx, filename in enumerate(merged_df['filename']):\n",
    "    seen_by = {}\n",
    "    for dataset in datasets.keys():\n",
    "        repetition_columns = [col for col in merged_df.columns if f'_reps_{dataset}' in col] \n",
    "        sub_reps = []\n",
    "        for col in repetition_columns:\n",
    "            if merged_df.loc[idx, col] > 0:\n",
    "                sub_reps.append(col)\n",
    "        if sub_reps:\n",
    "            seen_by.update({dataset: sub_reps})\n",
    "    if len(seen_by) > 1:\n",
    "        overlap.update({filename: seen_by})\n",
    "\n",
    "#summing up all individual unique stimuli across datasets will be 166,594.\n",
    "# Of those, 2829 are not unique. 2805 have been seen across exactly two \n",
    "# datasets and 24 have been seen across exactly 3 datasets.\n",
    "#Thus, there are 2853 duplicated 'filename' rows resulting in 163,741 unique stimuli in the compiled dataset (before exclusion of similar stim and resolution of test/train conflicts)\n",
    "print(f\"Found {len(overlap)} stimuli overlapping across datasets\") #found 2829\n",
    "assert(len(overlap) == 2829)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stim_only = 0\n",
    "train_stim_only = 0\n",
    "mix_stim = 0\n",
    "datasets = ['NSD','BMD','BOLD5000','THINGS','GOD','deeprecon','HAD','NOD']\n",
    "for filename, dset_dict in overlap.items():\n",
    "    merged_info = merged_df[merged_df['filename'] == filename]\n",
    "    test_or_train = [merged_info[f'test_train_{dset}'].item() for dset in datasets if not pd.isna(merged_info[f'test_train_{dset}'].item())]\n",
    "    if set(['test']) == set(test_or_train):\n",
    "        test_stim_only += 1\n",
    "    elif set(['train']) == set(test_or_train):\n",
    "        train_stim_only += 1\n",
    "    elif set(['test','train']) == set(test_or_train):\n",
    "        mix_stim += 1\n",
    "    else:\n",
    "        raise ValueError(f\"stim {filename} should be in either test or train or both if it overlaps.\")\n",
    "    if len(test_or_train) == 3:\n",
    "        print(f\"{filename}: {test_or_train}\")\n",
    "print(f\"Test stim only: {test_stim_only}\")\n",
    "print(f\"Train stim only: {train_stim_only}\")\n",
    "print(f\"Found in both test and train: {mix_stim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_count = {\"count_1\": 0, \"count_2\": 0, \"count_3\": 0, \"count_4+:0\": 0}\n",
    "for filename, seen_by in overlap.items():\n",
    "    if len(seen_by) == 1:\n",
    "        overlap_count['count_1'] += 1\n",
    "    elif len(seen_by) == 2:\n",
    "        overlap_count['count_2'] += 1\n",
    "    elif len(seen_by) == 3:\n",
    "        overlap_count['count_3'] += 1\n",
    "    elif len(seen_by) >= 4:\n",
    "        overlap_count['count_4+'] += 1\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid seen_by {seen_by}\")\n",
    "for k,v in overlap_count.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GOD and deeprecon should overlap by 1250 stimuli, and BOLD5000 and NSD should overlap by 1410 stimuli.\n",
    "#while other datasets have overlapping stimuli, these are the only two pairs I could identify that\n",
    "#published how many stimuli overlap. We use this to check our numbers against those.\n",
    "check_datasets = [(\"GOD\", \"deeprecon\"), (\"BOLD5000\", \"NSD\")]\n",
    "for pair in check_datasets:\n",
    "    overlap = {} #get the source of each filename and ensure no conflicts\n",
    "    for idx, filename in enumerate(merged_df['filename']):\n",
    "        seen_by = {}\n",
    "        for dataset in pair:\n",
    "            repetition_columns = [col for col in merged_df.columns if f'_reps_{dataset}' in col] \n",
    "            sub_reps = []\n",
    "            for col in repetition_columns:\n",
    "                if merged_df.loc[idx, col] > 0:\n",
    "                    sub_reps.append(col)\n",
    "            if sub_reps:\n",
    "                seen_by.update({dataset: sub_reps})\n",
    "        if len(seen_by) > 1:\n",
    "            overlap.update({filename: seen_by})\n",
    "    print(f\"Pair {pair}: Found {len(overlap)} overlapping stimuli\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reorder the dataframe for readability\n",
    "rep_pattern = r\"sub-.*_reps.*\"\n",
    "repetition_columns = [col for col in merged_df.columns if re.search(rep_pattern, col)]\n",
    "\n",
    "alias_pattern = r\"alias_.*\"\n",
    "alias_columns = [col for col in merged_df.columns if re.search(alias_pattern, col)]\n",
    "\n",
    "testtrain_pattern = r\"test_train_.*\"\n",
    "testtrain_columns = [col for col in merged_df.columns if re.search(testtrain_pattern, col)]\n",
    "desired_ordering = ['filename', 'source'] + alias_columns + testtrain_columns + repetition_columns\n",
    "\n",
    "merged_df = merged_df[desired_ordering]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"saving stimulus info file...\")\n",
    "merged_df.to_csv(os.path.join(dataset_root, \"MOSAIC\", \"stimuli\", \"datasets_stiminfo\", \"compiled_dataset_stiminfo.tsv\"), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MOSAIC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
