{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ac723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getenv('PYTHONPATH'))\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import squareform\n",
    "from net2brain.feature_extraction import FeatureExtractor, all_networks\n",
    "\n",
    "from net2brain.utils.download_datasets import DatasetAlgonauts_NSD\n",
    "import math\n",
    "\n",
    "#local imports\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "\n",
    "from src.utils.transforms import SelectROIs\n",
    "from src.encoding_exp.encoding_utils.models.model import RegressionAlexNet, EncoderMultiHead, Encoder, C8NonSteerableCNN\n",
    "from src.encoding_exp.neuralpredictors.layers.readouts import SpatialXFeatureLinear\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "import yaml\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "#local imports\n",
    "from src.utils.transforms import SelectROIs\n",
    "from src.encoding_exp.encoding_utils.models.model import RegressionAlexNet, EncoderMultiHead, Encoder, C8NonSteerableCNN, AlexNetCore\n",
    "from src.encoding_exp.neuralpredictors.layers.readouts import SpatialXFeatureLinear\n",
    "from src.encoding_exp.neuralpredictors.layers.readouts.factorized import FullLinearReadout\n",
    "from src.utils.dataset import StimulusDataset\n",
    "from src.utils.helpers import FilterDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36f58e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lowertriangular(rdm):\n",
    "    num_conditions = rdm.shape[0]\n",
    "    return rdm[np.triu_indices(num_conditions,1)]\n",
    "\n",
    "def visualize_RDM(rdm, savefig=False):\n",
    "    rdm_rank = stats.rankdata(get_lowertriangular(rdm))\n",
    "    rdm_rank_norm = rdm_rank/rdm_rank.max()\n",
    "    rdm_rank_square = squareform(rdm_rank_norm)\n",
    "    plt.imshow(rdm_rank_square, cmap='jet')\n",
    "    plt.colorbar()\n",
    "    if savefig:\n",
    "        plt.savefig(savefig)\n",
    "        \n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "def computeRDM(activations_dict, do_pca=True, normalize=True):\n",
    "    ncond = len(activations_dict)\n",
    "    embeddings = []\n",
    "    filenames = [] #order of filenames\n",
    "    for filename, activation in activations_dict.items():\n",
    "        embeddings.append(activation.flatten())\n",
    "        filenames.append(filename)\n",
    "    arr = np.array(embeddings)\n",
    "    if do_pca:\n",
    "        print(\"running pca\")\n",
    "        n_components=100\n",
    "        pca = PCA(n_components=n_components)\n",
    "        transformed_arr = pca.fit_transform(arr)\n",
    "        arr = transformed_arr[:, :n_components]\n",
    "    print(f\"array shape: {arr.shape}\")\n",
    "\n",
    "    rdm = 1 - cosine_similarity(arr)\n",
    "    \n",
    "    assert len(filenames) == ncond, f\"number of filenames is {len(filenames)}, should be {ncond}\"\n",
    "    assert rdm.shape == (ncond, ncond), f\"shape of rdm is {rdm.shape}, should be ({ncond}, {ncond})\"\n",
    "    if normalize:\n",
    "        rdm_rank = stats.rankdata(get_lowertriangular(rdm))\n",
    "        rdm_rank_norm = rdm_rank/rdm_rank.max()\n",
    "        rdm = squareform(rdm_rank_norm)\n",
    "\n",
    "    return filenames, rdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d6a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.getenv(\"PROJECT_ROOT\", \"/default/path/to/datasets\") #use default if DATASETS_ROOT env variable is not set.\n",
    "dataset_root = os.path.join(os.getenv(\"DATASETS_ROOT\", \"/default/path/to/datasets\"), \"MOSAIC\") #use default if DATASETS_ROOT env variable is not set.\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\" #use a different GPU than other programs\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ebfc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_paths = sorted(glob.glob(os.path.join(\"/data/vision/oliva/datasets/kingdaka/stimuli\", \"*.jpg\")))\n",
    "stimulus_paths.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b2ee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download dataset\n",
    "#NSD_dataset = DatasetAlgonauts_NSD()  # or use DatasetAlgonauts_NSD\n",
    "#paths = NSD_dataset.load_dataset(path=os.path.join(project_root, \"src\", \"stimulusSetPreparation\", \"extract_embeddings\", \"nsd_test_tmp\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a54038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from net2brain.feature_extraction import all_networks\n",
    "print(all_networks['Taskonomy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18be2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for netset in all_networks.keys():\n",
    "    if netset != 'Taskonomy':\n",
    "        continue\n",
    "    for model_name in all_networks[netset]:\n",
    "        fx_model = FeatureExtractor(model=model_name,\n",
    "                                    netset=netset,\n",
    "                                    device='cpu')\n",
    "        print(f\"{model_name}: {fx_model.get_all_layers()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8c90fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dict = defaultdict(defaultdict)\n",
    "for netset in all_networks.keys():\n",
    "    if netset != 'Taskonomy':\n",
    "        continue\n",
    "    for model_name in all_networks[netset]:\n",
    "        fx_model = FeatureExtractor(model=model_name,\n",
    "                                    netset=netset,\n",
    "                                    device='cpu')\n",
    "        fx_model.get_all_layers()\n",
    "        #layers_to_extract = [\"features.10\"] #[\"features.0\", \"features.1\", \"features.2\",  \"features.3\",  \"features.4\", \"features.5\",  \"features.6\", \"features.7\",  \"features.8\", \"features.9\",  \"features.10\",  \"features.11\", \"features.12\", \"classifier.0\", \"classifier.1\", \"classifier.2\", \"classifier.3\", \"classifier.4\", \"classifier.5\", \"classifier.6\"] #['layer1.2.conv3', 'layer2.7.conv3', 'layer3.35.conv3', 'layer4.2.conv3'] #['layer1.2.relu','layer2.3.relu', 'layer3.5.relu', 'layer4.2.relu',]\n",
    "        #fx_model.layers_to_extract = layers_to_extract\n",
    "        ft_path = f'feats3_{model_name}'\n",
    "        fx_model.extract(data_path=stimulus_paths,\n",
    "                        save_path=os.path.join(project_root, \"src\", \"encoding_exp\", \"net2brain\", ft_path),\n",
    "                        layers_to_extract=[\"layer4.2.conv3\"]) #layers_to_extract)\n",
    "        npzFiles = glob.glob(os.path.join(project_root, \"src\", \"encoding_exp\",  \"net2brain\", ft_path, \"*.npz\"))\n",
    "        for f in npzFiles:\n",
    "            d = np.load(f)\n",
    "            for stim in range(1,157):\n",
    "                data_dict[f\"{ft_path}\"][f\"{stim:03d}\"] = d[f\"{stim:03d}\"].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b0e40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = data_dict\n",
    "# Calculate the number of layers\n",
    "num_layers = len(res.keys())\n",
    "\n",
    "# Calculate optimal grid dimensions to make it as square as possible\n",
    "nrows = math.ceil(math.sqrt(num_layers))\n",
    "ncols = math.ceil(num_layers / nrows)\n",
    "\n",
    "# Create a figure with subplots in a grid\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*4, nrows*4))\n",
    "\n",
    "# Flatten the axes array for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Find global min and max for consistent color scaling\n",
    "all_rdms = []\n",
    "for layer in res.keys():\n",
    "    print(layer)\n",
    "    _, rdm = computeRDM(res[layer], do_pca=False, normalize=True)\n",
    "    all_rdms.append(rdm)\n",
    "    \n",
    "global_min = min(np.min(rdm) for rdm in all_rdms)\n",
    "global_max = max(np.max(rdm) for rdm in all_rdms)\n",
    "\n",
    "# Define tick positions and labels\n",
    "tick_positions = [27, 63, 99, 123, 155]\n",
    "tick_labels = [\"Animals\", \"Objects\", \"Scenes\", \"People\", \"Faces\"]\n",
    "\n",
    "# Plot each layer's RDM in a separate subplot\n",
    "for i, layer in enumerate(res.keys()):\n",
    "    if i >= nrows * ncols:  # Skip if we've run out of subplots\n",
    "        print(f\"Warning: Not enough subplots for layer {layer}. Increase nrows*ncols.\")\n",
    "        break\n",
    "        \n",
    "    rdm = all_rdms[i]\n",
    "    \n",
    "    # Plot the matrix with consistent color scaling\n",
    "    im = axes[i].imshow(rdm, cmap='jet', vmin=global_min, vmax=global_max)\n",
    "    \n",
    "    # Add title\n",
    "    axes[i].set_title(f'{layer} RDM')\n",
    "    axes[i].set_xlabel('X-axis')\n",
    "    axes[i].set_ylabel('Y-axis')\n",
    "    \n",
    "    # Set custom tick positions and labels\n",
    "    axes[i].set_xticks(tick_positions)\n",
    "    axes[i].set_yticks(tick_positions)\n",
    "    axes[i].set_xticklabels(tick_labels, rotation=45, ha='right')\n",
    "    axes[i].set_yticklabels(tick_labels)\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i+1, nrows*ncols):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "# Add a single colorbar for all subplots\n",
    "if num_layers > 0:  # Only add colorbar if we have at least one plot\n",
    "    # Adjust colorbar position based on grid size\n",
    "    cbar_width = 0.02\n",
    "    cbar_padding = 0.05\n",
    "    cbar_left = 1.0 - cbar_width - 0.01\n",
    "    cbar_height = 0.7\n",
    "    cbar_bottom = 0.15\n",
    "    \n",
    "    cbar_ax = fig.add_axes([cbar_left, cbar_bottom, cbar_width, cbar_height])  \n",
    "    cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "    cbar.set_label('Normalized Distance')\n",
    "    \n",
    "    # Adjust layout to prevent overlap - make room for colorbar\n",
    "    rect_right = 1.0 - (cbar_width + cbar_padding)\n",
    "    plt.tight_layout(rect=[0, 0, rect_right, 1])\n",
    "else:\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36002a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MOSAIC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
