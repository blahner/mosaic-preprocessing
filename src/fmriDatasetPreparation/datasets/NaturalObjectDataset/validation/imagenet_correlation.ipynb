{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getenv('PYTHONPATH')) \n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import hcp_utils as hcp\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import plotting\n",
    "\n",
    "#local\n",
    "from src.utils.helpers import vectorized_correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the test-retest reliability of imagenet image presentations. Correlate sessions within a participant, then average over the 9 participants. This is to validate our preprocessing by replicating NOD manuscript Figure 5a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = os.path.join(os.getenv(\"DATASETS_ROOT\", \"/default/path/to/datasets\"),\"NaturalObjectDataset\") #use default if DATASETS_ROOT env variable is not set.\n",
    "project_root = os.getenv(\"PROJECT_ROOT\", \"/default/path/to/project\")\n",
    "print(f\"dataset_root: {dataset_root}\")\n",
    "print(f\"project_root: {project_root}\")\n",
    "fmri_path = os.path.join(dataset_root,\"derivatives\", \"GLM\")\n",
    "task='imagenet'\n",
    "dataset = 'NOD'\n",
    "#imagenet\n",
    "with open(os.path.join(dataset_root,\"derivatives\", \"stimuli_metadata\", \"testtrain_split\", \"synset_words_edited.txt\"), 'r') as f:\n",
    "    # Initialize lists to store the columns\n",
    "    imagenet_names = []\n",
    "\n",
    "    # Iterate through each line in the file\n",
    "    for line in f:\n",
    "        # Split the line at the first space to get the 'n*' code and the labels\n",
    "        parts = line.strip().split(' ', 1)  # Split on first space only\n",
    "        imagenet_names.append(parts[0])  # First part is the imagenet name\n",
    "\n",
    "#get a list of imagenet filenames n*. order doesnt matter\n",
    "assert(len(imagenet_names) == 1000)\n",
    "subject_betas = {} #this will be a big dictionary holding all the beta estimates from the subjects\n",
    "n_subjects = 9\n",
    "nvertices = 91282\n",
    "session_group = \"sessiongroup-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_divisions = {\"session01\": range(10), \"session02\": range(10,20),\"session03\": range(20,30),\"session04\": range(30,40)}\n",
    "for sub in range(1, n_subjects+1):\n",
    "    session_conds = {\"session01\": [], \"session02\": [],\"session03\": [],\"session04\": []}\n",
    "    betas_tmp_session01 = {stim: [] for stim in imagenet_names}\n",
    "    betas_tmp_session02 = {stim: [] for stim in imagenet_names}\n",
    "    betas_tmp_session03 = {stim: [] for stim in imagenet_names}\n",
    "    betas_tmp_session04 = {stim: [] for stim in imagenet_names}\n",
    "\n",
    "    subject = f\"sub-{sub:02}\"    \n",
    "    fmri_data_wb = np.load(os.path.join(fmri_path, subject, session_group, \"TYPED_FITHRF_GLMDENOISE_RR.npy\"), allow_pickle=True).item()\n",
    "    fmri_data_wb = fmri_data_wb['betasmd'].squeeze() #squeezed to shape numvertices x numtrials\n",
    "    fmri_data_wb = fmri_data_wb.T #transpose to shape numtrials x numvertices, more representative of the samples x features format  \n",
    "    print(f\"shape of {subject} betas in the session (numtrials x numvertices): {fmri_data_wb.shape}\")\n",
    "  \n",
    "    with open(os.path.join(fmri_path, subject, session_group, f\"{subject}_{session_group}_conditionOrderDM.pkl\"), 'rb') as f:\n",
    "        events_run, ses_conds = pickle.load(f)\n",
    "\n",
    "    session01_conds = [] \n",
    "    session02_conds = [] \n",
    "    session03_conds = [] \n",
    "    session04_conds = [] \n",
    "    for run_number, event in enumerate(events_run):\n",
    "        stimuli = event['trial_type']\n",
    "        for stim in stimuli:\n",
    "            if 'imagenet' in stim:\n",
    "                filename = stim.split('/')[1]\n",
    "            elif 'coco' in stim:\n",
    "                tmp = stim.split('/')[-1]\n",
    "                filename = tmp.split('.')[0]\n",
    "\n",
    "            if run_number in session_divisions[\"session01\"]:\n",
    "                session01_conds.append(filename) #add an item to the list to preserve length\n",
    "                session02_conds.append(\"skip\") #add an item to the list to preserve length\n",
    "                session03_conds.append(\"skip\") #add an item to the list to preserve length\n",
    "                session04_conds.append(\"skip\") #add an item to the list to preserve length\n",
    "            elif run_number in session_divisions[\"session02\"]:\n",
    "                session01_conds.append(\"skip\")\n",
    "                session02_conds.append(filename) #add an item to the list to preserve length\n",
    "                session03_conds.append(\"skip\") #add an item to the list to preserve length\n",
    "                session04_conds.append(\"skip\") #add an item to the list to preserve length\n",
    "            elif run_number in session_divisions[\"session03\"]:\n",
    "                session01_conds.append(\"skip\")\n",
    "                session02_conds.append(\"skip\") #add an item to the list to preserve length\n",
    "                session03_conds.append(filename) #add an item to the list to preserve length\n",
    "                session04_conds.append(\"skip\") #add an item to the list to preserve length\n",
    "            elif run_number in session_divisions[\"session04\"]:\n",
    "                session01_conds.append(\"skip\")\n",
    "                session02_conds.append(\"skip\") #add an item to the list to preserve length\n",
    "                session03_conds.append(\"skip\") #add an item to the list to preserve length\n",
    "                session04_conds.append(filename) #add an item to the list to preserve length\n",
    "            else:\n",
    "                session01_conds.append(\"skip\")\n",
    "                session02_conds.append(\"skip\") #add an item to the list to preserve length\n",
    "                session03_conds.append(\"skip\") #add an item to the list to preserve length\n",
    "                session04_conds.append(\"skip\") #add an item to the list to preserve length        \n",
    "    session01_idx = np.isin(session01_conds, imagenet_names) #excludes coco filenames and 'skip' filenames.\n",
    "    session02_idx = np.isin(session02_conds, imagenet_names) #excludes coco filenames and 'skip' filenames.\n",
    "    session03_idx = np.isin(session03_conds, imagenet_names) #excludes coco filenames and 'skip' filenames.\n",
    "    session04_idx = np.isin(session04_conds, imagenet_names) #excludes coco filenames and 'skip' filenames.\n",
    "\n",
    "    fmri_data_wb_session01 = fmri_data_wb[session01_idx, :]\n",
    "    fmri_data_wb_session02 = fmri_data_wb[session02_idx, :]\n",
    "    fmri_data_wb_session03 = fmri_data_wb[session03_idx, :]\n",
    "    fmri_data_wb_session04 = fmri_data_wb[session04_idx, :]\n",
    "    \n",
    "    #no need to normalize since we are doing a pearson correlation\n",
    "    #average over repetitions\n",
    "    session01_count = 0\n",
    "    for session01_stim in session01_conds:\n",
    "        if session01_stim in imagenet_names:\n",
    "            betas_tmp_session01[session01_stim].append(fmri_data_wb_session01[session01_count,:])\n",
    "            session01_count += 1\n",
    "    session02_count = 0\n",
    "    for session02_stim in session02_conds:\n",
    "        if session02_stim in imagenet_names:\n",
    "            betas_tmp_session02[session02_stim].append(fmri_data_wb_session02[session02_count,:])\n",
    "            session02_count += 1\n",
    "    session03_count = 0\n",
    "    for session03_stim in session03_conds:\n",
    "        if session03_stim in imagenet_names:\n",
    "            betas_tmp_session03[session03_stim].append(fmri_data_wb_session03[session03_count,:])\n",
    "            session03_count += 1\n",
    "    session04_count = 0\n",
    "    for session04_stim in session04_conds:\n",
    "        if session04_stim in imagenet_names:\n",
    "            betas_tmp_session04[session04_stim].append(fmri_data_wb_session04[session04_count,:])\n",
    "            session04_count += 1\n",
    "\n",
    "    assert(session01_count == fmri_data_wb_session01.shape[0])\n",
    "    assert(session02_count == fmri_data_wb_session02.shape[0])\n",
    "    assert(session03_count == fmri_data_wb_session03.shape[0])\n",
    "    assert(session04_count == fmri_data_wb_session04.shape[0])\n",
    "\n",
    "    numreps = 1 #max number of reps for each session split\n",
    "    numvertices = 91282\n",
    "\n",
    "    betas_session01 = np.zeros((len(betas_tmp_session01), numreps, numvertices))\n",
    "    betas_session01.fill(np.nan)\n",
    "    betas_session02 = np.zeros((len(betas_tmp_session02), numreps, numvertices))\n",
    "    betas_session02.fill(np.nan)\n",
    "    betas_session03 = np.zeros((len(betas_tmp_session03), numreps, numvertices))\n",
    "    betas_session03.fill(np.nan)\n",
    "    betas_session04 = np.zeros((len(betas_tmp_session04), numreps, numvertices))\n",
    "    betas_session04.fill(np.nan)\n",
    "\n",
    "    stimorder_session01 = []\n",
    "    stimorder_session02 = []\n",
    "    stimorder_session03 = []\n",
    "    stimorder_session04 = []\n",
    "\n",
    "    for stimcount, b in enumerate(betas_tmp_session01.keys()):\n",
    "        value = betas_tmp_session01[b]\n",
    "        stimorder_session01.append(b)\n",
    "        for repcount, v in enumerate(value): #loop over reps\n",
    "            betas_session01[stimcount, repcount, :] = np.array(v)\n",
    "    for stimcount, b in enumerate(betas_tmp_session02.keys()):\n",
    "        value = betas_tmp_session02[b]\n",
    "        stimorder_session02.append(b)\n",
    "        for repcount, v in enumerate(value): #loop over reps\n",
    "            betas_session02[stimcount, repcount, :] = np.array(v)\n",
    "    for stimcount, b in enumerate(betas_tmp_session03.keys()):\n",
    "        value = betas_tmp_session03[b]\n",
    "        stimorder_session03.append(b)\n",
    "        for repcount, v in enumerate(value): #loop over reps\n",
    "            betas_session03[stimcount, repcount, :] = np.array(v)\n",
    "    for stimcount, b in enumerate(betas_tmp_session04.keys()):\n",
    "        value = betas_tmp_session04[b]\n",
    "        stimorder_session04.append(b)\n",
    "        for repcount, v in enumerate(value): #loop over reps\n",
    "            betas_session04[stimcount, repcount, :] = np.array(v)\n",
    "\n",
    "    assert(stimorder_session01 == stimorder_session02 == stimorder_session03 == stimorder_session04)\n",
    "\n",
    "    betas_session01_mean = np.nanmean(betas_session01, axis=1)\n",
    "    betas_session02_mean = np.nanmean(betas_session02, axis=1)\n",
    "    betas_session03_mean = np.nanmean(betas_session03, axis=1)\n",
    "    betas_session04_mean = np.nanmean(betas_session04, axis=1)\n",
    "\n",
    "    print(f\"Session01 betas shape: {betas_session01_mean.shape}\")\n",
    "    print(f\"Session02 betas shape: {betas_session02_mean.shape}\")\n",
    "    print(f\"Session03 betas shape: {betas_session03_mean.shape}\")\n",
    "    print(f\"Session04 betas shape: {betas_session04_mean.shape}\")\n",
    "\n",
    "    subject_betas.update({f\"{subject}_session01\": betas_session01_mean,\n",
    "                          f\"{subject}_session02\": betas_session02_mean,\n",
    "                          f\"{subject}_session03\": betas_session03_mean,\n",
    "                          f\"{subject}_session04\": betas_session04_mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reliability = np.zeros((nvertices,))\n",
    "splits = [(0,1), (0,2), (0,3), (1,2), (1,3), (2,3)]\n",
    "for sub in range(1,n_subjects+1):\n",
    "    subject_reliability = np.zeros((nvertices,))\n",
    "    subject = f\"sub-{sub:02}\"    \n",
    "    print(f\"running pairwise split correlation between sessions on subject: {subject}\")\n",
    "    for split in tqdm(splits):\n",
    "        betas_sessionA = subject_betas[f\"{subject}_session{split[0]+1:02}\"]\n",
    "        betas_sessionB = subject_betas[f\"{subject}_session{split[1]+1:02}\"]\n",
    "        #shuffled_indices = np.random.permutation(len(imagenet_names)) #another sanity check to make sure wer are capturing signal across categories\n",
    "        #subject_reliability += vectorized_correlation(betas_sessionA[shuffled_indices,:], betas_sessionB)\n",
    "        subject_reliability += vectorized_correlation(betas_sessionA, betas_sessionB, axis=0, ddof=1)\n",
    "    reliability += subject_reliability/len(splits)\n",
    "reliability = reliability/n_subjects #average the individual reliabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_list = ['png'] #only matters if save_flag is True \n",
    "\n",
    "save_flag=True #set to True to save plots or False to not save plots\n",
    "\n",
    "save_root = os.path.join(project_root, \"src\", \"fmriDatasetPreparation\", \"datasets\", \"NaturalObjectDataset\", \"validation\", \"output\", \"imagenet_reliability\")\n",
    "if not os.path.exists(save_root):\n",
    "    os.makedirs(save_root)\n",
    "\n",
    "views = ['lateral', 'ventral', 'medial'] #['lateral', 'medial', 'dorsal', 'ventral', 'anterior', 'posterior']\n",
    "stat = reliability.copy()\n",
    "print(f\"Min, Max Group Averaged Pairwise Session Reliability Pearson Correlation: {np.nanmin(stat)}, {np.nanmax(stat)}\")\n",
    "threshold = None\n",
    "cmap = 'coolwarm' #'hot'\n",
    "#save inflated surfaces\n",
    "cortex_data = hcp.cortex_data(stat)\n",
    "#determine global min/max for consistent color scaling\n",
    "datamin = np.nanmin(cortex_data)\n",
    "datamax = np.nanmax(cortex_data)\n",
    "vmin=-datamax #datamin\n",
    "vmax=datamax\n",
    "\n",
    "views = ['lateral', 'ventral', 'dorsal'] #['lateral', 'medial', 'dorsal', 'ventral', 'anterior', 'posterior']\n",
    "for hemi in ['left','right']:\n",
    "    mesh = hcp.mesh.inflated\n",
    "    bg = hcp.mesh.sulc\n",
    "    for view in views:\n",
    "        display = plotting.plot_surf_stat_map(mesh, cortex_data, hemi=hemi,\n",
    "        threshold=threshold, bg_map=bg, view=view, cmap=cmap)\n",
    "        if save_flag:\n",
    "            for ext in ext_list:\n",
    "                if ext == 'png':\n",
    "                    plt.savefig(os.path.join(save_root, f\"groupISC_{dataset}_task-{task}_mesh-inflated_view-{view}_hemi-{hemi}.{ext}\"),dpi=300)\n",
    "                else:\n",
    "                    plt.savefig(os.path.join(save_root, f\"groupISC_{dataset}_task-{task}_mesh-inflated_view-{view}_hemi-{hemi}.{ext}\"))\n",
    "\n",
    "#Save flat maps. hemispheres are combined in one plot\n",
    "#get the data for both hemispheres\n",
    "cortex_data_left = hcp.left_cortex_data(stat)\n",
    "cortex_data_right = hcp.right_cortex_data(stat)\n",
    "\n",
    "#create a figure with multiple axes to plot each anatomical image\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4), subplot_kw={'projection': '3d'})\n",
    "plt.subplots_adjust(wspace=0)\n",
    "im = plotting.plot_surf(hcp.mesh.flat_left, cortex_data_left,\n",
    "        threshold=threshold, bg_map=hcp.mesh.sulc_left, \n",
    "        colorbar=False, cmap=cmap, \n",
    "        vmin=vmin, vmax=vmax,\n",
    "        axes = axes[0])\n",
    "im = plotting.plot_surf(hcp.mesh.flat_right, cortex_data_right,\n",
    "        threshold=threshold, bg_map=hcp.mesh.sulc_right, \n",
    "        colorbar=False, cmap=cmap, \n",
    "        vmin=vmin, vmax=vmax,\n",
    "        axes = axes[1])\n",
    "\n",
    "#flip along the horizontal\n",
    "axes[0].invert_yaxis()\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "#create colorbar\n",
    "norm = plt.Normalize(vmin=vmin, vmax=vmax)\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = fig.colorbar(sm, ax=axes.ravel().tolist(), shrink=0.6)\n",
    "\n",
    "cbar.set_ticks([round(vmin,2), 0, round(vmax,2)])\n",
    "cbar.set_ticklabels([round(vmin,2), 0, round(vmax,2)])\n",
    "if save_flag:\n",
    "    for ext in ext_list:\n",
    "        if ext == 'png':\n",
    "            plt.savefig(os.path.join(save_root, f\"groupISC_{dataset}_task-{task}_mesh-flat.{ext}\"),dpi=300)\n",
    "        else:\n",
    "            plt.savefig(os.path.join(save_root, f\"groupISC_{dataset}_task-{task}_mesh-flat.{ext}\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MOSAIC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
