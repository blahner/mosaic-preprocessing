{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the test-retest reliability of COCO image presentations. Correlate odd and even runs withing a participant, then average over the 9 participants. This is to validate our preprocessing by replicating NOD manuscript Figure 5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getenv('PYTHONPATH')) \n",
    "import numpy as np\n",
    "import pickle\n",
    "import hcp_utils as hcp\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import plotting\n",
    "\n",
    "#local\n",
    "from src.utils.helpers import vectorized_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = os.path.join(os.getenv(\"DATASETS_ROOT\", \"/default/path/to/datasets\"),\"NaturalObjectDataset\") #use default if DATASETS_ROOT env variable is not set.\n",
    "project_root = os.getenv(\"PROJECT_ROOT\", \"/default/path/to/project\")\n",
    "print(f\"dataset_root: {dataset_root}\")\n",
    "print(f\"project_root: {project_root}\")\n",
    "fmri_path = os.path.join(dataset_root,\"derivatives\", \"GLM\")\n",
    "task='coco'\n",
    "dataset = 'NOD'\n",
    "#first define the stimulus order and matrix.\n",
    "#Next we will essentially place the betas into this pre-defined matrix\n",
    "with open(os.path.join(dataset_root, \"derivatives\", \"stimuli_metadata\", \"testtrain_split\", \"coco_groupings_rdm.pkl\"),'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "#get a list of coco filenames. order doesnt matter\n",
    "coco_filenames = [f for _,fname in data.items() for f in fname]\n",
    "assert(len(coco_filenames) == 120)\n",
    "subject_betas = {} #this will be a big dictionary holding all the beta estimates from the subjects\n",
    "n_subjects = 9\n",
    "nvertices = 91282\n",
    "session_group = \"sessiongroup-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in range(1,n_subjects+1):\n",
    "    betas_tmp_odd = {stim: [] for stim in coco_filenames}\n",
    "    betas_tmp_even = {stim: [] for stim in coco_filenames}\n",
    "    subject = f\"sub-{sub:02}\"    \n",
    "    fmri_data_wb = np.load(os.path.join(fmri_path, subject, session_group, \"TYPED_FITHRF_GLMDENOISE_RR.npy\"), allow_pickle=True).item()\n",
    "    fmri_data_wb = fmri_data_wb['betasmd'].squeeze() #squeezed to shape numvertices x numtrials\n",
    "    fmri_data_wb = fmri_data_wb.T #transpose to shape numtrials x numvertices, more representative of the samples x features format  \n",
    "    print(f\"shape of {subject} betas in the session (numtrials x numvertices): {fmri_data_wb.shape}\")\n",
    "  \n",
    "    with open(os.path.join(fmri_path, subject, session_group, f\"{subject}_{session_group}_conditionOrderDM.pkl\"), 'rb') as f:\n",
    "        events_run, ses_conds = pickle.load(f)\n",
    "    odd_conds = [] #even and odd conds list will include imagenet filenames too\n",
    "    even_conds = []\n",
    "    for run_number, event in enumerate(events_run):\n",
    "        stimuli = event['trial_type']\n",
    "        for stim in stimuli:\n",
    "            tmp = stim.split('/')[-1]\n",
    "            filename = tmp.split('.')[0]\n",
    "            if run_number % 2 == 0:\n",
    "                even_conds.append(filename)\n",
    "                odd_conds.append(\"skip\") #add an item to the list to preserve length, but the item is not one of the coco filenames\n",
    "            else:\n",
    "                odd_conds.append(filename)\n",
    "                even_conds.append(\"skip\")\n",
    "\n",
    "    odd_idx = np.isin(odd_conds, coco_filenames) #excludes imagenet filenames and 'skip' filenames.\n",
    "    even_idx = np.isin(even_conds, coco_filenames)\n",
    "\n",
    "    fmri_data_wb_odd = fmri_data_wb[odd_idx, :]\n",
    "    fmri_data_wb_even = fmri_data_wb[even_idx, :]\n",
    "    \n",
    "    #no need to normalize since we are doing a pearson correlation\n",
    "    #average over repetitions\n",
    "    odd_count = 0\n",
    "    for odd_stim in odd_conds:\n",
    "        if odd_stim in coco_filenames:\n",
    "            betas_tmp_odd[odd_stim].append(fmri_data_wb_odd[odd_count,:])\n",
    "            odd_count += 1\n",
    "    even_count = 0\n",
    "    for even_stim in even_conds:\n",
    "        if even_stim in coco_filenames:\n",
    "            betas_tmp_even[even_stim].append(fmri_data_wb_even[even_count,:])\n",
    "            even_count += 1\n",
    "\n",
    "    assert(odd_count == fmri_data_wb_odd.shape[0])\n",
    "    assert(even_count == fmri_data_wb_even.shape[0])    \n",
    "\n",
    "    numreps = 6 #max number of reps for a odd/even split\n",
    "    numvertices = 91282\n",
    "\n",
    "    #these matrices will be mainly nans because we use the numreps for coco images even on the imagenet ones\n",
    "    betas_odd = np.zeros((len(betas_tmp_odd), numreps, numvertices))\n",
    "    betas_odd.fill(np.nan)\n",
    "    betas_even = np.zeros((len(betas_tmp_even), numreps, numvertices))\n",
    "    betas_even.fill(np.nan)\n",
    "\n",
    "    stimorder_odd = []\n",
    "    stimorder_even = []\n",
    "    for stimcount, b in enumerate(betas_tmp_odd.keys()):\n",
    "        value = betas_tmp_odd[b]\n",
    "        stimorder_odd.append(b)\n",
    "        for repcount, v in enumerate(value): #loop over reps\n",
    "            betas_odd[stimcount, repcount, :] = np.array(v)\n",
    "    for stimcount, b in enumerate(betas_tmp_even.keys()):\n",
    "        value = betas_tmp_even[b]\n",
    "        stimorder_even.append(b)\n",
    "        for repcount, v in enumerate(value): #loop over reps\n",
    "            betas_even[stimcount, repcount, :] = np.array(v)\n",
    "    assert(stimorder_odd == stimorder_even)\n",
    "\n",
    "    betas_odd_mean = np.nanmean(betas_odd, axis=1)\n",
    "    betas_even_mean = np.nanmean(betas_even, axis=1)\n",
    "    print(f\"Odd run betas shape: {betas_odd_mean.shape}\")\n",
    "    print(f\"Even run betas shape: {betas_even_mean.shape}\")\n",
    "    subject_betas.update({f\"{subject}_odd\": betas_odd_mean, f\"{subject}_even\": betas_even_mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no longer needed with vectorized correlation\n",
    "nan_vertices_list = []\n",
    "for subject, betas in subject_betas.items():\n",
    "    sub_nans = np.argwhere(np.isnan(np.mean(betas, axis=0)))\n",
    "    nan_vertices_list.extend(sub_nans.flatten())\n",
    "    print(f\"subject {subject} has {len(sub_nans)} nans\")\n",
    "nan_vertices = set(nan_vertices_list)\n",
    "print(f\"all subjects have {len(nan_vertices)} unique nans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reliability = np.zeros((nvertices,))\n",
    "for sub in range(1,n_subjects+1):\n",
    "    subject = f\"sub-{sub:02}\"    \n",
    "    print(f\"running odd/even split correlation on subject: {subject}\")\n",
    "    betas_odd = subject_betas[f\"{subject}_odd\"]\n",
    "    betas_even = subject_betas[f\"{subject}_even\"]\n",
    "    #shuffled_indices = np.random.permutation(len(coco_filenames)) #another sanity check to make sure wer are capturing signal across categories\n",
    "    #reliability += vectorized_correlation(betas_odd[shuffled_indices,:], betas_even)\n",
    "    reliability += vectorized_correlation(betas_odd, betas_even, axis=0, ddof=1)\n",
    "\n",
    "reliability = reliability/n_subjects #average the individual reliabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_list = ['png'] #only matters if save_flag is True \n",
    "\n",
    "save_flag=True #set to True to save plots or False to not save plots\n",
    "\n",
    "save_root = os.path.join(project_root, \"src\", \"fmriDatasetPreparation\", \"datasets\", \"NaturalObjectDataset\", \"validation\", \"output\", \"coco_reliability\")\n",
    "if not os.path.exists(save_root):\n",
    "    os.makedirs(save_root)\n",
    "\n",
    "views = ['lateral', 'ventral', 'medial'] #['lateral', 'medial', 'dorsal', 'ventral', 'anterior', 'posterior']\n",
    "stat = reliability.copy()\n",
    "print(f\"Min, Max Group Averaged Odd/Even Reliability Pearson Correlation: {np.nanmin(stat)}, {np.nanmax(stat)}\")\n",
    "threshold = None\n",
    "cmap = 'coolwarm' #'hot'\n",
    "#save inflated surfaces\n",
    "cortex_data = hcp.cortex_data(stat)\n",
    "#determine global min/max for consistent color scaling\n",
    "datamin = np.nanmin(cortex_data)\n",
    "datamax = np.nanmax(cortex_data)\n",
    "vmin=-datamax #datamin\n",
    "vmax=datamax\n",
    "\n",
    "views = ['lateral', 'ventral', 'dorsal'] #['lateral', 'medial', 'dorsal', 'ventral', 'anterior', 'posterior']\n",
    "for hemi in ['left','right']:\n",
    "    mesh = hcp.mesh.inflated\n",
    "    bg = hcp.mesh.sulc\n",
    "    for view in views:\n",
    "        display = plotting.plot_surf_stat_map(mesh, cortex_data, hemi=hemi,\n",
    "        threshold=threshold, bg_map=bg, view=view, cmap=cmap)\n",
    "        if save_flag:\n",
    "            for ext in ext_list:\n",
    "                if ext == 'png':\n",
    "                    plt.savefig(os.path.join(save_root, f\"groupISC_{dataset}_task-{task}_mesh-inflated_view-{view}_hemi-{hemi}.{ext}\"),dpi=300)\n",
    "                else:\n",
    "                    plt.savefig(os.path.join(save_root, f\"groupISC_{dataset}_task-{task}_mesh-inflated_view-{view}_hemi-{hemi}.{ext}\"))\n",
    "\n",
    "#Save flat maps. hemispheres are combined in one plot\n",
    "#get the data for both hemispheres\n",
    "cortex_data_left = hcp.left_cortex_data(stat)\n",
    "cortex_data_right = hcp.right_cortex_data(stat)\n",
    "\n",
    "#create a figure with multiple axes to plot each anatomical image\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4), subplot_kw={'projection': '3d'})\n",
    "plt.subplots_adjust(wspace=0)\n",
    "im = plotting.plot_surf(hcp.mesh.flat_left, cortex_data_left,\n",
    "        threshold=threshold, bg_map=hcp.mesh.sulc_left, \n",
    "        colorbar=False, cmap=cmap, \n",
    "        vmin=vmin, vmax=vmax,\n",
    "        axes = axes[0])\n",
    "im = plotting.plot_surf(hcp.mesh.flat_right, cortex_data_right,\n",
    "        threshold=threshold, bg_map=hcp.mesh.sulc_right, \n",
    "        colorbar=False, cmap=cmap, \n",
    "        vmin=vmin, vmax=vmax,\n",
    "        axes = axes[1])\n",
    "\n",
    "#flip along the horizontal\n",
    "axes[0].invert_yaxis()\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "#create colorbar\n",
    "norm = plt.Normalize(vmin=vmin, vmax=vmax)\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = fig.colorbar(sm, ax=axes.ravel().tolist(), shrink=0.6)\n",
    "\n",
    "cbar.set_ticks([round(vmin,2), 0, round(vmax,2)])\n",
    "cbar.set_ticklabels([round(vmin,2), 0, round(vmax,2)])\n",
    "if save_flag:\n",
    "    for ext in ext_list:\n",
    "        if ext == 'png':\n",
    "            plt.savefig(os.path.join(save_root, f\"groupISC_{dataset}_task-{task}_mesh-flat.{ext}\"),dpi=300)\n",
    "        else:\n",
    "            plt.savefig(os.path.join(save_root, f\"groupISC_{dataset}_task-{task}_mesh-flat.{ext}\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MOSAIC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
