{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate that each imagenet session shows 1 unique image from all 1000 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = os.path.join(os.getenv(\"DATASETS_ROOT\", \"/default/path/to/datasets\"),\"NaturalObjectDataset\") #use default if DATASETS_ROOT env variable is not set.\n",
    "project_root = os.getenv(\"PROJECT_ROOT\", \"/default/path/to/project\")\n",
    "print(f\"dataset_root: {dataset_root}\")\n",
    "print(f\"project_root: {project_root}\")\n",
    "fmri_path = os.path.join(dataset_root,\"derivatives\", \"GLM\")\n",
    "task='imagenet'\n",
    "\n",
    "#first define the stimulus order and matrix.\n",
    "#Next we will essentially place the betas into this pre-defined matrix\n",
    "#imagenet\n",
    "with open(os.path.join(dataset_root,\"derivatives\", \"stimuli_metadata\", \"testtrain_split\", \"synset_words_edited.txt\"), 'r') as f:\n",
    "    # Initialize lists to store the columns\n",
    "    imagenet_names = []\n",
    "\n",
    "    # Iterate through each line in the file\n",
    "    for line in f:\n",
    "        # Split the line at the first space to get the 'n*' code and the labels\n",
    "        parts = line.strip().split(' ', 1)  # Split on first space only\n",
    "        imagenet_names.append(parts[0])  # First part is the imagenet name\n",
    "\n",
    "#get a list of imagenet filenames n*. order doesnt matter\n",
    "assert(len(imagenet_names) == 1000)\n",
    "subject_betas = {} #this will be a big dictionary holding all the beta estimates from the subjects\n",
    "\n",
    "nvertices = 91282\n",
    "session_group = \"sessiongroup-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allsubject_images = set() #keep track of all images shown across subjects and sessions\n",
    "allsubject_categories = set() #keep track of all categories shown across subjects and sessions\n",
    "\n",
    "for sub in range(1,31):\n",
    "    subject = f\"sub-{int(sub):02}\"\n",
    "    print('*'*20)\n",
    "    print(f\"starting subject {subject}\")\n",
    "\n",
    "    #search over all sessions to find the ones that include runs of the specified task\n",
    "    session_path = os.path.join(dataset_root, \"derivatives\", \"fmriprep\", subject)\n",
    "    sessions_tmp = sorted(glob.glob(os.path.join(session_path, f\"*imagenet*\"))) + sorted(glob.glob(os.path.join(session_path, f\"*coco*\")))  #compile all the session numbers\n",
    "    assert(len(sessions_tmp) > 0)\n",
    "    sessions = []\n",
    "    for s in sessions_tmp:\n",
    "        sname = s.split(\"/\")[-1]\n",
    "        if \"imagenet05\" not in sname:\n",
    "            sessions.append(sname)\n",
    "    assert(len(sessions) > 0)\n",
    "    print(f\"Found {len(sessions)} sessions\")\n",
    "    print(f\"{sessions}\")\n",
    "\n",
    "    allsessions_images = set() #keep track of all images shown across sessions\n",
    "    allsessions_categories = set() #keep track of all categories shown across sessions\n",
    "    \n",
    "    for session_count, session_path in enumerate(sessions):\n",
    "        allruns_images = set()  #keep track of all images shown across the runs in a sessions for one subject\n",
    "        allruns_categories = set()#keep track of all categories shown across the runs in a sessions for one subject\n",
    "        session = session_path.split('/')[-1]\n",
    "        if 'coco' in session:\n",
    "            continue\n",
    "        elif 'imagenet' in session:\n",
    "            task='imagenet'\n",
    "            events_stim_field = 'stim_file'\n",
    "        else:\n",
    "            raise ValueError(\"Invalid task name. Must be either coco or imagenet session.\")\n",
    "        \n",
    "        numruns = len(glob.glob(os.path.join(dataset_root, \"derivatives\", \"fmriprep\", subject, session, \"func\", f\"{subject}_{session}_task-{task}_run-*_desc-confounds_timeseries.tsv\")))  #nothing special about the confounds file choice\n",
    "        assert(numruns > 0)\n",
    "        print(f\"Found {numruns} runs for subject {subject} session {session}\")\n",
    "\n",
    "        ##Load eventts and data for each run\n",
    "        for count, run in enumerate(range(1,numruns+1)):\n",
    "            #print(\"run:\",run)\n",
    "            #load events\n",
    "            tmp = pd.read_table(os.path.join(dataset_root, \"Nifti\", subject, session, \"func\", f\"{subject}_{session}_task-{task}_run-{run:02}_events.tsv\"))\n",
    "            for idx, stim in enumerate(tmp.loc[:,events_stim_field]):\n",
    "                if str(stim) == 'nan': #for coco subject 1, blank trials are not input into the events file. for subjects 2-9, they are listed as n/a conditions with their own onsets\n",
    "                    continue\n",
    "                if 'imagenet' in stim:\n",
    "                    imagenet_category = stim.split('/')[1]\n",
    "                    imagenet_filename = stim.split('/')[2]\n",
    "                elif 'coco' in stim:\n",
    "                    tmp = stim.split('/')[-1]\n",
    "                    coco_filename = tmp.split('.')[0]\n",
    "                allruns_images.add(imagenet_filename)\n",
    "                allruns_categories.add(imagenet_category)\n",
    "        print(f\"Number of unique imagenet images in session {session}: {len(allruns_images)}\")\n",
    "        print(f\"Number of unique imagenet categories in session {session}: {len(allruns_categories)}\")\n",
    "        assert(len(allruns_images) == 1000)\n",
    "        assert(len(allruns_categories) == 1000)\n",
    "\n",
    "        allsessions_images.update(allruns_images)\n",
    "        allsessions_categories.update(allruns_categories)\n",
    "    if sub < 10:\n",
    "        assert(len(allsessions_images) == 4000)\n",
    "        assert(len(allsessions_categories) == 1000)\n",
    "    else:\n",
    "        assert(len(allsessions_images) == 1000)\n",
    "        assert(len(allsessions_categories) == 1000)\n",
    "\n",
    "    allsubject_images.update(allsessions_images)\n",
    "    allsubject_categories.update(allsessions_categories)\n",
    "    \n",
    "print(f\"Number of unique imagenet images across all sessions: {len(allsubject_images)}\")\n",
    "print(f\"Number of unique imagenet categories across all sessions: {len(allsubject_categories)}\")\n",
    "assert(len(allsubject_images) == 57000)\n",
    "assert(len(allsubject_categories) == 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuroAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
